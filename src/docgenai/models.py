"""
Model implementations for DocGenAI.

This module provides model classes for different backends (MLX and Transformers)
with automatic platform detection and optimized configurations.
"""

import logging
import os
import platform
import sys
import time
import warnings
from abc import ABC, abstractmethod
from contextlib import contextmanager
from io import StringIO
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

# Suppress MLX deprecation warnings for cleaner output
warnings.filterwarnings("ignore", message=".*mx.metal.* is deprecated.*")

logger = logging.getLogger(__name__)


@contextmanager
def suppress_stderr():
    """Context manager to temporarily suppress stderr output."""
    with open(os.devnull, "w") as devnull:
        old_stderr = sys.stderr
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stderr = old_stderr


class AIModel(ABC):
    """Abstract base class for AI models used in documentation generation."""

    @abstractmethod
    def generate_documentation(self, code: str, file_path: str, **kwargs) -> str:
        """Generate documentation for the given code."""
        pass

    @abstractmethod
    def generate_architecture_description(
        self, code: str, file_path: str, **kwargs
    ) -> str:
        """Generate architecture description for the given code."""
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """Check if the model is available and ready to use."""
        pass

    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the model."""
        pass

    @staticmethod
    def _get_platform_info():
        """Get platform information for model selection."""
        system = platform.system()
        machine = platform.machine()

        # Check if MLX is available (for Apple Silicon, even in Docker)
        mlx_available = False
        try:
            import mlx.core as mx

            mlx_available = True
        except ImportError:
            mlx_available = False

        # Platform selection logic:
        # 1. If MLX is available and we're on ARM64, use MLX
        # 2. Otherwise, use transformers
        if mlx_available and machine in ["arm64", "aarch64"]:
            return "mlx"
        else:
            return "transformers"


class DeepSeekCoderModel(AIModel):
    """
    DeepSeek-Coder-V2-Lite model implementation with platform detection.

    - macOS: Uses mlx-lm with DeepSeek-Coder-V2-Lite-Instruct-8bit
    - Linux/Windows: Uses transformers with DeepSeek-Coder-V2-Lite-Instruct
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize DeepSeek-Coder model with platform detection."""
        self.config = config or {}
        self.platform = platform.system()
        self.model = None
        self.tokenizer = None
        self.is_mac = self.platform == "Darwin"

        # Get model configuration
        model_config = self.config.get("model", {})

        # Model paths based on platform and config
        if self.is_mac:
            self.model_path = model_config.get(
                "mlx_model", "mlx-community/DeepSeek-Coder-V2-Lite-Instruct-8bit"
            )
            self.backend = "mlx"
        else:
            self.model_path = model_config.get(
                "transformers_model", "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
            )
            self.backend = "transformers"

        # Generation parameters from config
        self.temperature = model_config.get("temperature", 0.7)
        self.max_tokens = model_config.get("max_tokens", 2048)
        self.top_p = model_config.get("top_p", 0.8)
        self.top_k = model_config.get("top_k", 50)
        self.do_sample = model_config.get("do_sample", True)

        # Hardware optimization settings
        self.device_map = model_config.get("device_map", "auto")
        self.torch_dtype = model_config.get("torch_dtype", "auto")
        self.trust_remote_code = model_config.get("trust_remote_code", True)
        self.quantization = model_config.get("quantization", "4bit")

        logger.info(f"üñ•Ô∏è  Platform detected: {self.platform}")
        logger.info(f"ü§ñ Model backend: {self.backend}")
        logger.info(f"üìç Model path: {self.model_path}")

        self._initialize_model()

    def _initialize_model(self):
        """Initialize the model based on platform."""
        logger.info("üîß Initializing DeepSeek-Coder model...")
        start_time = time.time()

        try:
            if self.is_mac:
                self._initialize_mlx_model()
            else:
                self._initialize_transformers_model()

            elapsed = time.time() - start_time
            logger.info(
                f"üéâ Model initialization complete! "
                f"Total time: {elapsed:.2f} seconds"
            )

        except Exception as e:
            logger.error(f"‚ùå Model initialization failed: {str(e)}")
            raise

    def _initialize_mlx_model(self):
        """Initialize MLX model for macOS."""
        logger.info("üì• Step 1/3: Loading MLX model and tokenizer...")

        try:
            from mlx_lm import generate, load

            self.mlx_generate = generate

            # Load model and tokenizer
            logger.info(f"üì¶ Loading {self.model_path}...")
            self.model, self.tokenizer = load(self.model_path)
            logger.info("‚úÖ MLX model loaded successfully")

        except ImportError as e:
            logger.error("‚ùå mlx-lm not available. Install with: pip install mlx-lm")
            raise ImportError(
                "mlx-lm is required for macOS. " "Install with: pip install mlx-lm"
            ) from e
        except Exception as e:
            logger.error(f"‚ùå Failed to load MLX model: {str(e)}")
            raise

    def _initialize_transformers_model(self):
        """Initialize transformers model for non-macOS platforms."""
        logger.info("üì• Step 1/4: Loading transformers model...")

        try:
            import torch
            from transformers import (
                AutoModelForCausalLM,
                AutoTokenizer,
            )

            # Check if this is an AWQ model (already quantized)
            is_awq_model = "awq" in self.model_path.lower()

            if is_awq_model:
                logger.info(
                    "‚öôÔ∏è  Detected AWQ model - loading without additional quantization"
                )
                # AWQ models are already quantized, load them directly
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                )
            else:
                # For non-AWQ models, use 4-bit quantization if requested
                logger.info(f"‚öôÔ∏è  Setting up {self.quantization} quantization...")

                if self.quantization == "4bit":
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        load_in_4bit=True,
                        device_map="auto",
                        torch_dtype=torch.float16,
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                    )
                elif self.quantization == "8bit":
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        load_in_8bit=True,
                        device_map="auto",
                        torch_dtype=torch.float16,
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        device_map="auto",
                        torch_dtype=torch.float16,
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                    )

            logger.info("üìù Step 2/4: Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, trust_remote_code=True
            )

            # Ensure tokenizer has pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            logger.info("‚úÖ Model and tokenizer loaded successfully")

        except Exception as e:
            logger.error(f"Failed to load transformers model: {e}")
            raise

    def _generate_with_mlx(self, prompt: str, max_tokens: int = None) -> str:
        """Generate text using MLX backend."""
        try:
            from mlx_lm import sample_utils

            max_tokens = max_tokens or self.max_tokens

            # Create sampler using make_sampler function
            sampler = sample_utils.make_sampler(
                temp=self.temperature,
                top_p=self.top_p,
                min_p=0.0,
                top_k=self.top_k if hasattr(self, "top_k") else -1,
            )

            # Suppress MLX deprecation warnings during generation
            with suppress_stderr():
                response = self.mlx_generate(
                    self.model,
                    self.tokenizer,
                    prompt=prompt,
                    max_tokens=max_tokens,
                    sampler=sampler,
                )
            return response
        except Exception as e:
            logger.error(f"‚ùå MLX generation failed: {str(e)}")
            raise

    def _generate_with_transformers(self, prompt: str, max_tokens: int = None) -> str:
        """Generate text using transformers backend."""
        try:
            import torch

            # Apply chat template for instruction-tuned model
            messages = [{"role": "user", "content": prompt}]

            # Use the tokenizer's chat template
            if hasattr(self.tokenizer, "apply_chat_template"):
                formatted_prompt = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                # Fallback to simple format
                formatted_prompt = f"User: {prompt}\n\nAssistant:"

            inputs = self.tokenizer.encode(formatted_prompt, return_tensors="pt")
            if torch.cuda.is_available() and hasattr(self.model, "device"):
                inputs = inputs.to(self.model.device)

            # Update generation config with any overrides
            gen_config = self.generation_config.copy()
            if max_tokens:
                gen_config["max_new_tokens"] = max_tokens

            with torch.no_grad():
                outputs = self.model.generate(inputs, **gen_config)

            # Decode only the new tokens
            response = self.tokenizer.decode(
                outputs[0][len(inputs[0]) :], skip_special_tokens=True
            )
            return response.strip()

        except Exception as e:
            logger.error(f"‚ùå Transformers generation failed: {str(e)}")
            raise

    def _generate_text(self, prompt: str, max_tokens: int = None) -> str:
        """Generate text using appropriate backend."""
        logger.info("üîÑ Running model inference...")
        start_time = time.time()

        try:
            if self.is_mac:
                response = self._generate_with_mlx(prompt, max_tokens)
            else:
                response = self._generate_with_transformers(prompt, max_tokens)

            elapsed = time.time() - start_time
            logger.info(f"‚úÖ Generation complete in {elapsed:.2f} seconds")

            return response

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"‚ùå Generation failed after {elapsed:.2f} seconds: {str(e)}")
            raise

    def generate_documentation(self, code: str, file_path: str, **kwargs) -> str:
        """Generate comprehensive documentation for the given code."""
        logger.info(f"üìù Generating documentation for {file_path}")

        # Detect programming language
        file_extension = Path(file_path).suffix.lower()
        language = self._detect_language(file_extension)

        # Create documentation prompt
        prompt = f"""You are an expert software developer and technical writer. Generate comprehensive, clear, and well-structured documentation for the following {language} code.

The documentation should include:

1. **Overview**: Brief description of what the code does
2. **Key Components**: Main classes, functions, and their purposes
3. **Architecture**: How the components work together
4. **Usage Examples**: Practical examples of how to use the code
5. **Dependencies**: Any external libraries or modules used
6. **Configuration**: Any configuration options or environment variables
7. **Error Handling**: How errors are handled and common issues
8. **Performance Considerations**: Any performance notes or optimizations

Please write the documentation in clear, professional Markdown format following these rules:
- Use ## headers (not # headers) for main sections
- Surround all lists with blank lines before and after
- Use only single blank lines between sections
- Specify language for all code blocks (```python, ```bash, etc.)
- Do NOT wrap your entire response in a code block
- Do NOT use duplicate section headings

**File Path**: `{file_path}`

**Code**:
```{language}
{code}
```

Write the documentation directly (no code block wrapper):"""

        return self._generate_text(prompt)

    def generate_architecture_description(
        self, code: str, file_path: str, **kwargs
    ) -> str:
        """Generate architectural analysis for the given code."""
        logger.info(f"üèóÔ∏è  Generating architecture description for {file_path}")

        file_extension = Path(file_path).suffix.lower()
        language = self._detect_language(file_extension)

        prompt = f"""You are a software architect analyzing code structure. Provide a detailed architectural analysis of the following {language} code.

Focus on:

1. **Architectural Patterns**: Design patterns used (MVC, Observer, Factory, etc.)
2. **Code Organization**: How the code is structured and organized
3. **Data Flow**: How data moves through the system
4. **Dependencies**: Internal and external dependencies
5. **Interfaces**: Public APIs and interfaces exposed
6. **Extensibility**: How the code can be extended or modified
7. **Design Principles**: SOLID principles, separation of concerns, etc.
8. **Potential Improvements**: Suggestions for architectural improvements

Follow markdown formatting rules:
- Use ## headers for main sections, ### for subsections
- Surround all lists with blank lines before and after
- Use only single blank lines between sections
- Avoid duplicate section headings

**File Path**: `{file_path}`

**Code**:
```{language}
{code}
```

Provide the architectural analysis:"""

        return self._generate_text(prompt)

    def _detect_language(self, file_extension: str) -> str:
        """Detect programming language from file extension."""
        language_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".jsx": "jsx",
            ".tsx": "tsx",
            ".cpp": "cpp",
            ".cc": "cpp",
            ".cxx": "cpp",
            ".c": "c",
            ".h": "c",
            ".hpp": "cpp",
            ".java": "java",
            ".go": "go",
            ".rs": "rust",
            ".rb": "ruby",
            ".php": "php",
            ".cs": "csharp",
            ".swift": "swift",
            ".kt": "kotlin",
            ".scala": "scala",
            ".r": "r",
        }
        return language_map.get(file_extension.lower(), "text")

    def is_available(self) -> bool:
        """Check if the model is available and ready to use."""
        return self.model is not None and self.tokenizer is not None

    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the model."""
        return {
            "model_path": self.model_path,
            "backend": self.backend,
            "platform": self.platform,
            "is_mac": self.is_mac,
            "available": self.is_available(),
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "quantization": self.quantization if not self.is_mac else "8bit (MLX)",
        }


def create_model(config: Optional[Dict[str, Any]] = None) -> AIModel:
    """
    Create an AI model instance based on platform and configuration.

    Args:
        config: Optional configuration dictionary

    Returns:
        AIModel instance (DeepSeekCoderModel)
    """
    logger.info("üè≠ Creating AI model instance...")

    try:
        model = DeepSeekCoderModel(config)
        logger.info(
            f"‚úÖ Created {model.__class__.__name__} with {model.backend} backend"
        )
        return model
    except Exception as e:
        logger.error(f"‚ùå Failed to create model: {str(e)}")
        raise


# Backward compatibility aliases
class DeepSeekV3Model(DeepSeekCoderModel):
    """Backward compatibility alias for DeepSeekCoderModel."""

    pass


class MMaDAModel(DeepSeekCoderModel):
    """Backward compatibility wrapper for old MMaDA model usage."""

    def __init__(self, model_name: str = None, **kwargs):
        logger.warning("‚ö†Ô∏è  MMaDAModel is deprecated. Use DeepSeekCoderModel instead.")
        super().__init__(**kwargs)
